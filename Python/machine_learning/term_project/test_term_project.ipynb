{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 데이터 로드 중\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 1199.65it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 1124.24it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 1199.07it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 1960.00it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 2503.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images data : (200, 150, 150, 3), dtype = float32"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Images category : (200,), dtype = int32\n"
     ]
    }
   ],
   "source": [
    "# 1 - Data labeling\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "IMAGE_SIZE = (150, 150)\n",
    "\n",
    "# 데이터 로드\n",
    "def load_data(data_dir):\n",
    "    dataset = 'train'\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # 클래스 이름과 라벨을 얻기\n",
    "    class_names_label = {name: idx for idx, name in enumerate(os.listdir(os.path.join(data_dir, dataset)))}\n",
    "\n",
    "    print(\"{} 데이터 로드 중\".format(dataset))\n",
    "\n",
    "    dataset_path = os.path.join(data_dir, dataset)\n",
    "\n",
    "    # 각 카테고리에 해당하는 폴더를 순회\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        label = class_names_label[folder]\n",
    "\n",
    "        # 폴더 안의 각 이미지를 순회\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        for file in tqdm(os.listdir(folder_path)):\n",
    "            # 이미지의 경로 이름을 얻기\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "\n",
    "            # 이미지 열기 및 리사이즈\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is not None:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, IMAGE_SIZE)\n",
    "\n",
    "                # 이미지와 해당 라벨을 출력 리스트에 추가\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "    images = np.array(images, dtype='float32')\n",
    "    labels = np.array(labels, dtype='int32')\n",
    "\n",
    "    return images, labels, class_names_label\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "data_dir = \"C:/Coding/Python/machine_learning/term_project\"\n",
    "\n",
    "# 데이터 로드\n",
    "images, labels, class_names_label = load_data(data_dir)\n",
    "\n",
    "# 확인 출력\n",
    "print(f'Images data : {images.shape}, dtype = {images.dtype}')\n",
    "print(f'Images category : {labels.shape}, dtype = {labels.dtype}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m images_normalized \u001b[38;5;241m=\u001b[39m normalize(images)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# 평균 필터 적용\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m images_filtered \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([average_filter(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images_normalized])\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 데이터 증강 (3배)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m images_augmented, labels_augmented \u001b[38;5;241m=\u001b[39m augment_data(images_filtered, labels)\n",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     66\u001b[0m images_normalized \u001b[38;5;241m=\u001b[39m normalize(images)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# 평균 필터 적용\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m images_filtered \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43maverage_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images_normalized])\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 데이터 증강 (3배)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m images_augmented, labels_augmented \u001b[38;5;241m=\u001b[39m augment_data(images_filtered, labels)\n",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36maverage_filter\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(padded_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]):\n\u001b[0;32m     18\u001b[0m             region \u001b[38;5;241m=\u001b[39m padded_data[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m, j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m, k]\n\u001b[1;32m---> 19\u001b[0m             filtered_data[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, k] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filtered_data\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jmw31\\anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3465\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jmw31\\anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\_methods.py:165\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mean\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 165\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     rcount \u001b[38;5;241m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 - Data Preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 정규화 (0-1 범위)\n",
    "def normalize(data):\n",
    "    return data / 255.0\n",
    "\n",
    "# 평균 필터\n",
    "def average_filter(data):\n",
    "    padded_data = np.pad(data, ((1, 1), (1, 1), (0, 0)), mode='constant', constant_values=0)\n",
    "    filtered_data = np.zeros_like(data)\n",
    "    \n",
    "    for i in range(1, padded_data.shape[0] - 1):\n",
    "        for j in range(1, padded_data.shape[1] - 1):\n",
    "            for k in range(padded_data.shape[2]):\n",
    "                region = padded_data[i-1:i+2, j-1:j+2, k]\n",
    "                filtered_data[i-1, j-1, k] = np.mean(region)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "# 데이터 증강\n",
    "def augment_data(images, labels):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for img, label in zip(images, labels):\n",
    "        # Original\n",
    "        augmented_images.append(img)\n",
    "        augmented_labels.append(label)\n",
    "        \n",
    "        # Horizontal flip\n",
    "        h_flip = cv2.flip(img, 1)\n",
    "        augmented_images.append(h_flip)\n",
    "        augmented_labels.append(label)\n",
    "        \n",
    "        # Vertical flip\n",
    "        v_flip = cv2.flip(img, 0)\n",
    "        augmented_images.append(v_flip)\n",
    "        augmented_labels.append(label)\n",
    "    \n",
    "    return np.array(augmented_images), np.array(augmented_labels)\n",
    "\n",
    "# 데이터 셔플링\n",
    "def shuffle_data(images, labels):\n",
    "    indices = np.arange(images.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return images[indices], labels[indices]\n",
    "\n",
    "# 전처리 결과 확인\n",
    "def display_examples(class_names, images, labels):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fig.suptitle(\"Some examples of images of the dataset\", fontsize=16)\n",
    "\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "        plt.xlabel(class_names[labels[i]])\n",
    "    plt.show()\n",
    "\n",
    "# 데이터 정규화\n",
    "images_normalized = normalize(images)\n",
    "\n",
    "# 평균 필터 적용\n",
    "images_filtered = np.array([average_filter(img) for img in images_normalized])\n",
    "\n",
    "# 데이터 증강 (3배)\n",
    "images_augmented, labels_augmented = augment_data(images_filtered, labels)\n",
    "\n",
    "# 데이터 셔플링\n",
    "images_shuffled, labels_shuffled = shuffle_data(images_augmented, labels_augmented)\n",
    "\n",
    "# 전처리된 데이터 형태 출력\n",
    "print(f'Images size : {images_shuffled.shape}, dtype = {labels_shuffled.dtype}')\n",
    "\n",
    "# 학습 셋과 테스트 셋으로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_shuffled, labels_shuffled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"=========================================================\")\n",
    "\n",
    "# 전처리된 훈련 및 테스트 데이터 형태 출력\n",
    "print(f'Train Images size : {X_train.shape}, dtype = {X_train.dtype}')\n",
    "print(f'Test Images size : {X_test.shape}, dtype = {X_test.dtype}')\n",
    "\n",
    "print(\"=========================================================\")\n",
    "\n",
    "print(f'Train Labels size : {y_train.shape}, dtype = {y_train.dtype}')\n",
    "print(f'Test Labels size : {y_test.shape}, dtype = {y_test.dtype}')\n",
    "\n",
    "# 전처리된 데이터 일부 이미지 출력\n",
    "class_names = {idx: name for name, idx in class_names_label.items()}\n",
    "display_examples(class_names, images_shuffled, labels_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# CNN 모델 구성\n",
    "def create_cnn_model(input_shape, dropout_rates):\n",
    "    model = tf.keras.Sequential([\n",
    "        # First convolutional layer\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(dropout_rates[0]),\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(dropout_rates[1]),\n",
    "        \n",
    "        # Flatten and fully connected layer\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout_rates[2]),\n",
    "        \n",
    "        # Output layer\n",
    "        tf.keras.layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "dropout_rates_list = [\n",
    "    [0.1, 0.1, 0.3],\n",
    "    [0.2, 0.2, 0.2],\n",
    "    [0.2, 0.2, 0.4],\n",
    "    [0.2, 0.2, 0.5],\n",
    "    [0.3, 0.3, 0.5],\n",
    "    [0.4, 0.4, 0.5],\n",
    "    [0.5, 0.5, 0.5],\n",
    "]\n",
    "\n",
    "batch_sizes = [16, 32, 64, 128]  # 배치 사이즈 후보\n",
    "\n",
    "input_shape = (150, 150, 3)  # 이미지 크기와 채널 수 (RGB)\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "\n",
    "for dropout_rates in dropout_rates_list:\n",
    "    for batch_size in batch_sizes:\n",
    "        model = create_cnn_model(input_shape, dropout_rates)\n",
    "        \n",
    "        # 모델 컴파일\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # 모델 학습\n",
    "        history = model.fit(X_train, y_train, epochs=30, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "        \n",
    "        # 모델 평가\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "        \n",
    "        # 결과 저장\n",
    "        results.append({\n",
    "            'dropout_rates': dropout_rates,\n",
    "            'batch_size': batch_size,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc\n",
    "        })\n",
    "\n",
    "# 최적의 드롭아웃 비율과 배치 사이즈 찾기\n",
    "best_result = max(results, key=lambda x: x['test_acc'])\n",
    "\n",
    "# 결과 출력\n",
    "for result in results:\n",
    "    print(f\"Dropout Rates: {result['dropout_rates']}, Batch Size: {result['batch_size']}, Train Accuracy: {result['train_acc']:.4f}, Test Accuracy: {result['test_acc']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Dropout Rates:\", best_result['dropout_rates'])\n",
    "print(\"Best Batch Size:\", best_result['batch_size'])\n",
    "print(f\"Best Train Accuracy: {best_result['train_acc']:.4f}, Best Test Accuracy: {best_result['test_acc']:.4f}\")\n",
    "\n",
    "# 최적의 드롭아웃 비율과 배치 사이즈로 다시 모델 학습 및 저장\n",
    "best_model = create_cnn_model(input_shape, best_result['dropout_rates'])\n",
    "best_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train, epochs=30, batch_size=best_result['batch_size'], validation_split=0.2)\n",
    "\n",
    "# 모델 저장 경로 지정\n",
    "save_path = 'C:/Coding/Python/machine_learning/term_project/models/2020144030_JMW.h5'\n",
    "\n",
    "best_model.save('2020144030_JMW_best.h5')\n",
    "print(\"모델이 '2020144030_JMW_best.h5' 파일로 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
